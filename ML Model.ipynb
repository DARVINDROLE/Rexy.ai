{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX9LtPpbP4gR"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHAT BOT"
      ],
      "metadata": {
        "id": "cQlgBNUPQAMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # free, no API key\n",
        "\n",
        "# ===============================\n",
        "# Groq API setup\n",
        "# ===============================\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key=\"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\",\n",
        "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Helper Functions\n",
        "# ===============================\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf.strip())\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text()\n",
        "    return text\n",
        "\n",
        "\n",
        "def get_text_chunks(text):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=20\n",
        "    )\n",
        "    chunks = text_splitter.split_text(text)\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def get_vector_store(text_chunks):\n",
        "    # Use HuggingFace embeddings (local, no API needed)\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    vector_store = FAISS.from_texts(text_chunks, embedding=embeddings)\n",
        "    return vector_store\n",
        "\n",
        "\n",
        "def get_conversational_chain(vector_store):\n",
        "    memory = ConversationBufferMemory(\n",
        "        memory_key=\"chat_history\",\n",
        "        return_messages=True\n",
        "    )\n",
        "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
        "        llm=llm,\n",
        "        retriever=vector_store.as_retriever(),\n",
        "        memory=memory\n",
        "    )\n",
        "    return conversation_chain\n",
        "\n",
        "\n",
        "# ===============================\n",
        "# CLI Chat Functions\n",
        "# ===============================\n",
        "def user_input(conversation, user_question):\n",
        "    response = conversation({'question': user_question})\n",
        "    chat_history = response['chat_history']\n",
        "    for i, message in enumerate(chat_history):\n",
        "        if i % 2 == 0:\n",
        "            print(\"User:\", message.content)\n",
        "        else:\n",
        "            print(\"Reply:\", message.content)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Ask user for PDF path(s)\n",
        "    pdf_paths = input(\"Enter PDF file paths (comma separated): \").split(\",\")\n",
        "\n",
        "    # Load and process PDF(s)\n",
        "    raw_text = get_pdf_text(pdf_paths)\n",
        "    text_chunks = get_text_chunks(raw_text)\n",
        "    vector_store = get_vector_store(text_chunks)\n",
        "\n",
        "    # Create conversation chain\n",
        "    conversation = get_conversational_chain(vector_store)\n",
        "\n",
        "    print(\"‚úÖ System is ready! Ask questions (type 'exit' to quit).\\n\")\n",
        "\n",
        "    # Chat loop\n",
        "    while True:\n",
        "        user_question = input(\"You: \")\n",
        "        if user_question.lower() in [\"exit\", \"quit\"]:\n",
        "            print(\"üëã Exiting...\")\n",
        "            break\n",
        "        user_input(conversation, user_question)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "sTdWLf-XQGFd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-8wNLCcQNOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "FLASHCARD"
      ],
      "metadata": {
        "id": "o78_-eOjQO_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings  # free, no API key\n",
        "from langchain.vectorstores import FAISS\n",
        "import os\n",
        "import random\n",
        "\n",
        "# ===============================\n",
        "# Groq API setup\n",
        "# ===============================\n",
        "llm = ChatGroq(\n",
        "    temperature=0.3,\n",
        "    groq_api_key=\"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\",\n",
        "    model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Helper Functions\n",
        "# ===============================\n",
        "def get_pdf_text(pdf_docs):\n",
        "    text = \"\"\n",
        "    for pdf in pdf_docs:\n",
        "        pdf_reader = PdfReader(pdf.strip())\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text, chunk_size=1000, overlap=100):\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def generate_flashcards_from_chunk(chunk, num_cards=2):\n",
        "    prompt = f\"\"\"\n",
        "    Create {num_cards} flashcards (question-answer pairs) from the following text.\n",
        "    Format strictly as JSON list like:\n",
        "    [\n",
        "      {{\"question\": \"Q1?\", \"answer\": \"A1\"}},\n",
        "      {{\"question\": \"Q2?\", \"answer\": \"A2\"}}\n",
        "    ]\n",
        "\n",
        "    Text:\n",
        "    {chunk}\n",
        "    \"\"\"\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        return eval(response.content)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error generating flashcards from chunk: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_flashcards(text, final_count=5):\n",
        "    chunks = get_text_chunks(text)\n",
        "    flashcards = []\n",
        "\n",
        "    # collect small batches from each chunk\n",
        "    for chunk in chunks[:5]:  # limit chunks to avoid overload\n",
        "        flashcards.extend(generate_flashcards_from_chunk(chunk, num_cards=2))\n",
        "\n",
        "    # randomly pick exactly `final_count` cards\n",
        "    if len(flashcards) > final_count:\n",
        "        flashcards = random.sample(flashcards, final_count)\n",
        "\n",
        "    return flashcards\n",
        "\n",
        "# ===============================\n",
        "# Main\n",
        "# ===============================\n",
        "def main():\n",
        "    pdf_paths = [\"/content/NOTES UNIT-4 ANN.pdf\"]\n",
        "\n",
        "    raw_text = get_pdf_text(pdf_paths)\n",
        "\n",
        "    print(\"‚è≥ Generating 5 flashcards...\")\n",
        "    flashcards = generate_flashcards(raw_text, final_count=5)\n",
        "\n",
        "    print(\"\\n‚úÖ Flashcards Generated!\\n\")\n",
        "    for i, card in enumerate(flashcards, 1):\n",
        "        print(f\"{i}. Q: {card['question']}\")\n",
        "        print(f\"   A: {card['answer']}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "6Bmcrw3YQNfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "QUIZ"
      ],
      "metadata": {
        "id": "DgAzLBDfQXDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "\n",
        "# ===============================\n",
        "# Groq API setup\n",
        "# ===============================\n",
        "llm = ChatGroq(\n",
        "    temperature=0.3,\n",
        "    groq_api_key=\"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\",\n",
        "    model_name=\"meta-llama/llama-4-maverick-17b-128e-instruct\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Helper Functions\n",
        "# ===============================\n",
        "def get_pdf_text(pdf_paths):\n",
        "    \"\"\"Extract text from PDF files\"\"\"\n",
        "    text = \"\"\n",
        "    for pdf_path in pdf_paths:\n",
        "        try:\n",
        "            if os.path.exists(pdf_path):\n",
        "                pdf_reader = PdfReader(pdf_path)\n",
        "                for page in pdf_reader.pages:\n",
        "                    text += page.extract_text() or \"\"\n",
        "            else:\n",
        "                print(f\"‚ö†Ô∏è PDF file not found: {pdf_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error reading PDF {pdf_path}: {e}\")\n",
        "    return text\n",
        "\n",
        "def get_text_chunks(text, chunk_size=1000, overlap=100):\n",
        "    \"\"\"Split text into manageable chunks\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=overlap\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def clean_response(raw):\n",
        "    \"\"\"Clean and parse JSON response from LLM\"\"\"\n",
        "    try:\n",
        "        # Remove backticks and code block markers\n",
        "        raw = raw.strip()\n",
        "        if raw.startswith(\"```json\"):\n",
        "            raw = raw.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "        elif raw.startswith(\"```\"):\n",
        "            raw = raw.replace(\"```\", \"\").strip()\n",
        "\n",
        "        # Try to find JSON array in the response\n",
        "        start_idx = raw.find('[')\n",
        "        end_idx = raw.rfind(']') + 1\n",
        "\n",
        "        if start_idx != -1 and end_idx != 0:\n",
        "            json_str = raw[start_idx:end_idx]\n",
        "            return json.loads(json_str)\n",
        "        else:\n",
        "            return json.loads(raw)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Failed to parse response: {e}\")\n",
        "        print(f\"Raw response: {raw[:200]}...\")\n",
        "        return []\n",
        "\n",
        "def generate_quiz_from_chunk(chunk, num_questions=2):\n",
        "    \"\"\"Generate quiz questions from a text chunk\"\"\"\n",
        "    if len(chunk.strip()) < 100:  # Skip very short chunks\n",
        "        return []\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "Create {num_questions} multiple-choice quiz questions from the following text.\n",
        "Each question must have exactly 4 options (A, B, C, D) and clearly indicate the correct answer.\n",
        "\n",
        "IMPORTANT: Format your response as a valid JSON array only, like this example:\n",
        "[\n",
        "    {{\n",
        "        \"question\": \"What is the main concept discussed?\",\n",
        "        \"options\": {{\n",
        "            \"A\": \"Option 1\",\n",
        "            \"B\": \"Option 2\",\n",
        "            \"C\": \"Option 3\",\n",
        "            \"D\": \"Option 4\"\n",
        "        }},\n",
        "        \"answer\": \"B\"\n",
        "    }}\n",
        "]\n",
        "\n",
        "Text to create questions from:\n",
        "{chunk[:800]}\n",
        "\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = llm.invoke(prompt)\n",
        "        questions = clean_response(response.content)\n",
        "\n",
        "        # Validate questions format\n",
        "        valid_questions = []\n",
        "        for q in questions:\n",
        "            if (isinstance(q, dict) and\n",
        "                'question' in q and\n",
        "                'options' in q and\n",
        "                'answer' in q and\n",
        "                isinstance(q['options'], dict) and\n",
        "                len(q['options']) == 4 and\n",
        "                q['answer'] in ['A', 'B', 'C', 'D']):\n",
        "                valid_questions.append(q)\n",
        "\n",
        "        return valid_questions\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error generating quiz from chunk: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_quiz(text, final_count=5):\n",
        "    \"\"\"Generate complete quiz from text\"\"\"\n",
        "    if not text.strip():\n",
        "        print(\"‚ö†Ô∏è No text extracted from PDFs\")\n",
        "        return []\n",
        "\n",
        "    print(f\"üìÑ Extracted {len(text)} characters from PDF(s)\")\n",
        "\n",
        "    chunks = get_text_chunks(text)\n",
        "    print(f\"üìö Created {len(chunks)} text chunks\")\n",
        "\n",
        "    quiz = []\n",
        "    chunks_used = 0\n",
        "\n",
        "    # Try to get questions from multiple chunks\n",
        "    for chunk in chunks[:min(10, len(chunks))]:  # Try up to 10 chunks\n",
        "        if len(quiz) >= final_count:\n",
        "            break\n",
        "\n",
        "        chunk_questions = generate_quiz_from_chunk(chunk, num_questions=1)\n",
        "        if chunk_questions:\n",
        "            quiz.extend(chunk_questions)\n",
        "            chunks_used += 1\n",
        "            print(f\"‚úÖ Generated {len(chunk_questions)} question(s) from chunk {chunks_used}\")\n",
        "\n",
        "    # Shuffle and limit to final count\n",
        "    if len(quiz) > final_count:\n",
        "        quiz = random.sample(quiz, final_count)\n",
        "\n",
        "    return quiz\n",
        "\n",
        "# ===============================\n",
        "# Main Interactive Quiz\n",
        "# ===============================\n",
        "def main():\n",
        "    print(\"üéØ PDF Quiz Generator\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # You can modify these paths or make them user input\n",
        "    pdf_paths = [\"/content/NOTES UNIT-4 ANN.pdf\"]\n",
        "\n",
        "    print(\"üìñ Reading PDF files...\")\n",
        "    raw_text = get_pdf_text(pdf_paths)\n",
        "\n",
        "    if not raw_text.strip():\n",
        "        print(\"‚ùå No text could be extracted from the PDFs. Please check the file paths.\")\n",
        "        return\n",
        "\n",
        "    print(\"‚è≥ Generating Quiz Questions...\")\n",
        "    quiz = generate_quiz(raw_text, final_count=5)\n",
        "\n",
        "    if not quiz:\n",
        "        print(\"‚ùå No quiz questions could be generated. Please check your PDF content and API connection.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\n‚úÖ Quiz Ready! Generated {len(quiz)} questions üöÄ\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    score = 0\n",
        "    total_questions = len(quiz)\n",
        "\n",
        "    for i, q in enumerate(quiz, 1):\n",
        "        print(f\"\\nQuestion {i}/{total_questions}:\")\n",
        "        print(f\"{q['question']}\")\n",
        "        print()\n",
        "\n",
        "        # Display options\n",
        "        for opt_key, opt_val in q['options'].items():\n",
        "            print(f\"  {opt_key}) {opt_val}\")\n",
        "\n",
        "        print()\n",
        "\n",
        "        # Get user answer with validation\n",
        "        while True:\n",
        "            user_ans = input(\"Your answer (A/B/C/D): \").strip().upper()\n",
        "            if user_ans in ['A', 'B', 'C', 'D']:\n",
        "                break\n",
        "            print(\"‚ö†Ô∏è Please enter A, B, C, or D\")\n",
        "\n",
        "        # Check answer\n",
        "        if user_ans == q['answer']:\n",
        "            print(\"‚úÖ Correct!\")\n",
        "            score += 1\n",
        "        else:\n",
        "            correct_option = q['options'][q['answer']]\n",
        "            print(f\"‚ùå Wrong! Correct Answer: {q['answer']}) {correct_option}\")\n",
        "\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "    # Final score\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"üéØ Quiz Completed!\")\n",
        "    print(f\"Your Final Score: {score}/{total_questions}\")\n",
        "\n",
        "    percentage = (score / total_questions) * 100\n",
        "    if percentage >= 80:\n",
        "        print(\"üèÜ Excellent work!\")\n",
        "    elif percentage >= 60:\n",
        "        print(\"üëç Good job!\")\n",
        "    elif percentage >= 40:\n",
        "        print(\"üìö Keep studying!\")\n",
        "    else:\n",
        "        print(\"üí™ Don't give up, practice makes perfect!\")\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "a9eQ6zeKQZdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PLANNED TIMETABLE"
      ],
      "metadata": {
        "id": "SZGr-BRnQdjA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cos4AlxRQhZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pod cast"
      ],
      "metadata": {
        "id": "aq0TW8nPQhsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import tempfile\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from gtts import gTTS\n",
        "import pyttsx3\n",
        "from pydub import AudioSegment\n",
        "\n",
        "# ===============================\n",
        "# 1. Load PDF and Extract Text\n",
        "# ===============================\n",
        "def get_pdf_text(pdf_path):\n",
        "    \"\"\"Extract text from PDF file\"\"\"\n",
        "    try:\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading PDF: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# ===============================\n",
        "# 2. Split Text into Chunks\n",
        "# ===============================\n",
        "def get_chunks(text, chunk_size=1000, chunk_overlap=200):\n",
        "    \"\"\"Split text into manageable chunks\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "# ===============================\n",
        "# 3. Enhanced Podcast Generator\n",
        "# ===============================\n",
        "def generate_podcast(text_chunk, llm):\n",
        "    \"\"\"Original podcast generator\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are a podcast script writer.\n",
        "    Convert the following study material into a podcast-style script\n",
        "    between two people: Host and Guest.\n",
        "\n",
        "    - The Host asks friendly, curious questions.\n",
        "    - The Guest explains clearly with examples, analogies, and stories.\n",
        "    - Keep it conversational, engaging, and easy to follow.\n",
        "    - Do not just read the text, make it sound like real dialogue.\n",
        "    - Format it clearly with Host: and Guest: labels for each speaker.\n",
        "\n",
        "    Text:\n",
        "    {text_chunk}\n",
        "    \"\"\"\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def generate_enhanced_podcast(text_chunk, llm, chunk_number=1, total_chunks=1):\n",
        "    \"\"\"Enhanced podcast script generator with better conversation flow\"\"\"\n",
        "    prompt = f\"\"\"\n",
        "    You are creating an educational podcast script. Convert this study material into\n",
        "    natural dialogue between Sarah (Host) and Dr. Alex (Subject Expert).\n",
        "\n",
        "    GUIDELINES:\n",
        "    - Sarah asks thoughtful questions and provides transitions\n",
        "    - Dr. Alex explains concepts clearly with real-world examples\n",
        "    - Include natural conversation elements: \"That's interesting...\", \"So what you're saying is...\"\n",
        "    - Break down complex topics into digestible parts\n",
        "    - Add brief recap/preview if this is part {chunk_number} of {total_chunks}\n",
        "    - Keep sentences conversational (not too long or academic)\n",
        "    - Include pauses like \"Um, let me think about that...\" for naturalness\n",
        "\n",
        "    STRUCTURE:\n",
        "    - Start with Sarah introducing the topic\n",
        "    - Use follow-up questions: \"Can you elaborate on that?\"\n",
        "    - End with Sarah summarizing key points\n",
        "    - Format as **Host:** and **Guest:** for clear speaker identification\n",
        "\n",
        "    Study Material:\n",
        "    {text_chunk}\n",
        "\n",
        "    Create an engaging 3-4 minute podcast segment.\n",
        "    \"\"\"\n",
        "\n",
        "    response = llm.invoke(prompt)\n",
        "    return response.content\n",
        "\n",
        "def add_podcast_intro_outro(script, topic_title=\"Study Session\"):\n",
        "    \"\"\"Add professional intro and outro to the podcast\"\"\"\n",
        "    intro = f\"\"\"**Host:** Welcome to StudyCast, where we turn your textbooks into conversations!\n",
        "I'm Sarah, and today we're diving into {topic_title}.\n",
        "With me is Dr. Alex, who's going to help us break this down. Ready to learn? Let's get started!\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "    outro = \"\"\"\n",
        "\n",
        "**Host:** That was really insightful, Dr. Alex! Thanks for breaking that down for us.\n",
        "**Guest:** My pleasure, Sarah. I hope this helps with your studies!\n",
        "**Host:** And thank you for listening to StudyCast. Keep learning, and we'll catch you in the next episode!\n",
        "\"\"\"\n",
        "\n",
        "    return intro + script + outro\n",
        "\n",
        "# ===============================\n",
        "# 4. Clean Script for TTS\n",
        "# ===============================\n",
        "def clean_script_for_tts(script):\n",
        "    \"\"\"Remove speaker labels and formatting marks from the podcast script\"\"\"\n",
        "    # Remove bold formatting markers\n",
        "    cleaned = re.sub(r'\\*\\*([^*]+)\\*\\*', r'\\1', script)\n",
        "\n",
        "    # Remove speaker labels (Host:, Guest:, etc.)\n",
        "    cleaned = re.sub(r'^(Host|Guest|Speaker \\d+):\\s*', '', cleaned, flags=re.MULTILINE)\n",
        "\n",
        "    # Remove any remaining markdown formatting\n",
        "    cleaned = re.sub(r'[*_`#]', '', cleaned)\n",
        "\n",
        "    # Clean up extra whitespace and newlines\n",
        "    cleaned = re.sub(r'\\n\\s*\\n', '\\n\\n', cleaned)\n",
        "    cleaned = cleaned.strip()\n",
        "\n",
        "    return cleaned\n",
        "\n",
        "# ===============================\n",
        "# 5. Text-to-Speech Methods\n",
        "# ===============================\n",
        "def text_to_speech_with_voices(text, output_file=\"podcast_episode.wav\"):\n",
        "    \"\"\"Uses pyttsx3 for different male/female voices\"\"\"\n",
        "    clean_text = clean_script_for_tts(text)\n",
        "\n",
        "    engine = pyttsx3.init()\n",
        "    voices = engine.getProperty('voices')\n",
        "\n",
        "    if not voices:\n",
        "        print(\"‚ùå No voices found. Using default voice.\")\n",
        "        engine.save_to_file(clean_text, output_file)\n",
        "        engine.runAndWait()\n",
        "        return\n",
        "\n",
        "    # Find male and female voices\n",
        "    male_voice = None\n",
        "    female_voice = None\n",
        "\n",
        "    for voice in voices:\n",
        "        voice_name = voice.name.lower()\n",
        "        if any(keyword in voice_name for keyword in ['male', 'david', 'mark', 'alex']):\n",
        "            male_voice = voice.id\n",
        "        elif any(keyword in voice_name for keyword in ['female', 'zira', 'susan', 'samantha']):\n",
        "            female_voice = voice.id\n",
        "\n",
        "    # Fallback to first two voices if gender-specific not found\n",
        "    if not male_voice:\n",
        "        male_voice = voices[0].id\n",
        "    if not female_voice:\n",
        "        female_voice = voices[1].id if len(voices) > 1 else voices[0].id\n",
        "\n",
        "    print(f\"üé≠ Using voices - Male: {male_voice}, Female: {female_voice}\")\n",
        "\n",
        "    engine.setProperty('voice', male_voice)\n",
        "    engine.setProperty('rate', 160)  # Adjust speech rate\n",
        "    engine.save_to_file(clean_text, output_file)\n",
        "    engine.runAndWait()\n",
        "\n",
        "    print(f\"‚úÖ Podcast audio saved as {output_file}\")\n",
        "\n",
        "def create_dual_voice_podcast(script, output_file=\"podcast_episode.wav\"):\n",
        "    \"\"\"Create podcast with separate male (Host) and female (Guest) voices\"\"\"\n",
        "    # Parse the script to separate Host and Guest parts\n",
        "    parts = []\n",
        "    current_text = \"\"\n",
        "    current_speaker = None\n",
        "\n",
        "    lines = script.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('**Host:**'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'host'\n",
        "            current_text = line.replace('**Host:**', '').strip()\n",
        "        elif line.startswith('**Guest:**'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'guest'\n",
        "            current_text = line.replace('**Guest:**', '').strip()\n",
        "        elif line.startswith('Host:'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'host'\n",
        "            current_text = line.replace('Host:', '').strip()\n",
        "        elif line.startswith('Guest:'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'guest'\n",
        "            current_text = line.replace('Guest:', '').strip()\n",
        "        else:\n",
        "            if line:\n",
        "                current_text += ' ' + line\n",
        "\n",
        "    # Add the last part\n",
        "    if current_text and current_speaker:\n",
        "        parts.append((current_speaker, current_text.strip()))\n",
        "\n",
        "    # Initialize TTS engine\n",
        "    engine = pyttsx3.init()\n",
        "    voices = engine.getProperty('voices')\n",
        "\n",
        "    if not voices:\n",
        "        print(\"‚ùå No voices available. Using single voice.\")\n",
        "        text_to_speech_with_voices(script, output_file)\n",
        "        return\n",
        "\n",
        "    # Set up voices\n",
        "    male_voice = voices[0].id\n",
        "    female_voice = voices[1].id if len(voices) > 1 else voices[0].id\n",
        "\n",
        "    print(f\"üé≠ Male voice (Host): {voices[0].name}\")\n",
        "    print(f\"üé≠ Female voice (Guest): {voices[1].name if len(voices) > 1 else 'Same as host'}\")\n",
        "\n",
        "    # Create temporary audio files for each part\n",
        "    temp_files = []\n",
        "\n",
        "    try:\n",
        "        for i, (speaker, text) in enumerate(parts):\n",
        "            if not text.strip():\n",
        "                continue\n",
        "\n",
        "            temp_file = tempfile.NamedTemporaryFile(suffix='.wav', delete=False)\n",
        "            temp_files.append(temp_file.name)\n",
        "\n",
        "            # Set voice based on speaker\n",
        "            if speaker == 'host':\n",
        "                engine.setProperty('voice', male_voice)\n",
        "                engine.setProperty('rate', 160)  # Slightly faster for host\n",
        "            else:  # guest\n",
        "                engine.setProperty('voice', female_voice)\n",
        "                engine.setProperty('rate', 150)  # Slightly slower for guest\n",
        "\n",
        "            # Generate audio for this part\n",
        "            engine.save_to_file(text, temp_file.name)\n",
        "            engine.runAndWait()\n",
        "\n",
        "            print(f\"‚úÖ Generated {speaker} part {i+1}\")\n",
        "\n",
        "        # Combine all audio segments\n",
        "        combined_audio = AudioSegment.empty()\n",
        "        for temp_file in temp_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                segment = AudioSegment.from_wav(temp_file)\n",
        "                combined_audio += segment\n",
        "                combined_audio += AudioSegment.silent(duration=500)  # Add 0.5s pause\n",
        "\n",
        "        combined_audio.export(output_file, format=\"wav\")\n",
        "        print(f\"‚úÖ Final podcast saved as {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating dual voice podcast: {e}\")\n",
        "        print(\"üí° Falling back to single voice...\")\n",
        "        text_to_speech_with_voices(script, output_file)\n",
        "\n",
        "    finally:\n",
        "        # Clean up temporary files\n",
        "        for temp_file in temp_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                try:\n",
        "                    os.unlink(temp_file)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "def create_gtts_dual_voice_podcast(script, output_file=\"podcast_gtts.mp3\"):\n",
        "    \"\"\"Uses gTTS with different accents to simulate different voices\"\"\"\n",
        "    # Parse script\n",
        "    parts = []\n",
        "    current_text = \"\"\n",
        "    current_speaker = None\n",
        "\n",
        "    lines = script.split('\\n')\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line.startswith('**Host:**'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'host'\n",
        "            current_text = line.replace('**Host:**', '').strip()\n",
        "        elif line.startswith('**Guest:**'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'guest'\n",
        "            current_text = line.replace('**Guest:**', '').strip()\n",
        "        elif line.startswith('Host:'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'host'\n",
        "            current_text = line.replace('Host:', '').strip()\n",
        "        elif line.startswith('Guest:'):\n",
        "            if current_text and current_speaker:\n",
        "                parts.append((current_speaker, current_text.strip()))\n",
        "            current_speaker = 'guest'\n",
        "            current_text = line.replace('Guest:', '').strip()\n",
        "        else:\n",
        "            if line:\n",
        "                current_text += ' ' + line\n",
        "\n",
        "    if current_text and current_speaker:\n",
        "        parts.append((current_speaker, current_text.strip()))\n",
        "\n",
        "    # Create audio segments\n",
        "    temp_files = []\n",
        "\n",
        "    try:\n",
        "        for i, (speaker, text) in enumerate(parts):\n",
        "            if not text.strip():\n",
        "                continue\n",
        "\n",
        "            temp_file = tempfile.NamedTemporaryFile(suffix='.mp3', delete=False)\n",
        "            temp_files.append(temp_file.name)\n",
        "\n",
        "            # Use different TTS settings for different voices\n",
        "            if speaker == 'host':\n",
        "                # Male voice simulation - use British English\n",
        "                tts = gTTS(text=text, lang='en', tld='co.uk', slow=False)\n",
        "            else:  # guest\n",
        "                # Female voice simulation - use Australian English\n",
        "                tts = gTTS(text=text, lang='en', tld='com.au', slow=False)\n",
        "\n",
        "            tts.save(temp_file.name)\n",
        "            print(f\"‚úÖ Generated {speaker} part {i+1}\")\n",
        "\n",
        "        # Combine audio files\n",
        "        combined_audio = AudioSegment.empty()\n",
        "        for temp_file in temp_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                segment = AudioSegment.from_mp3(temp_file)\n",
        "                combined_audio += segment\n",
        "                combined_audio += AudioSegment.silent(duration=800)  # Add pause\n",
        "\n",
        "        combined_audio.export(output_file, format=\"mp3\")\n",
        "        print(f\"‚úÖ Final podcast saved as {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error combining audio: {e}\")\n",
        "        print(\"üí° Try installing: pip install pydub\")\n",
        "        # Fallback to single voice\n",
        "        clean_text = clean_script_for_tts(script)\n",
        "        tts = gTTS(text=clean_text, lang=\"en\")\n",
        "        tts.save(output_file)\n",
        "        print(f\"‚úÖ Single voice fallback saved as {output_file}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up\n",
        "        for temp_file in temp_files:\n",
        "            if os.path.exists(temp_file):\n",
        "                try:\n",
        "                    os.unlink(temp_file)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "def create_background_music_podcast(script, output_file=\"podcast_with_music.mp3\"):\n",
        "    \"\"\"Add subtle background music to the podcast\"\"\"\n",
        "    try:\n",
        "        # Create the main audio first\n",
        "        create_gtts_dual_voice_podcast(script, \"temp_podcast.mp3\")\n",
        "\n",
        "        # Load the podcast\n",
        "        podcast = AudioSegment.from_mp3(\"temp_podcast.mp3\")\n",
        "\n",
        "        # Add a simple background tone (you could replace this with actual music)\n",
        "        # This creates a very subtle background hum\n",
        "        background = AudioSegment.sine(440, duration=len(podcast)).apply_gain(-30)\n",
        "\n",
        "        # Mix the audio\n",
        "        final_podcast = podcast.overlay(background)\n",
        "        final_podcast.export(output_file, format=\"mp3\")\n",
        "\n",
        "        # Clean up\n",
        "        if os.path.exists(\"temp_podcast.mp3\"):\n",
        "            os.unlink(\"temp_podcast.mp3\")\n",
        "\n",
        "        print(f\"‚úÖ Podcast with background audio saved as {output_file}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Background music failed: {e}\")\n",
        "        print(\"üí° Creating podcast without background music...\")\n",
        "        create_gtts_dual_voice_podcast(script, output_file)\n",
        "\n",
        "# ===============================\n",
        "# 6. Podcast Series Generation\n",
        "# ===============================\n",
        "def create_full_podcast_series(pdf_path, llm, max_chunks=5):\n",
        "    \"\"\"Create a complete podcast series from a PDF\"\"\"\n",
        "    # Load and process PDF\n",
        "    raw_text = get_pdf_text(pdf_path)\n",
        "    if not raw_text:\n",
        "        print(\"‚ùå Could not extract text from PDF\")\n",
        "        return []\n",
        "\n",
        "    chunks = get_chunks(raw_text, chunk_size=1500, chunk_overlap=300)\n",
        "\n",
        "    # Limit chunks for demo\n",
        "    chunks = chunks[:max_chunks]\n",
        "\n",
        "    print(f\"üìö Creating {len(chunks)} podcast episodes...\")\n",
        "\n",
        "    all_episodes = []\n",
        "\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"\\nüéôÔ∏è Generating Episode {i+1}/{len(chunks)}...\")\n",
        "\n",
        "        try:\n",
        "            # Generate enhanced script\n",
        "            episode_script = generate_enhanced_podcast(\n",
        "                chunk, llm, chunk_number=i+1, total_chunks=len(chunks)\n",
        "            )\n",
        "\n",
        "            # Add intro/outro\n",
        "            full_script = add_podcast_intro_outro(\n",
        "                episode_script,\n",
        "                topic_title=f\"Study Material - Part {i+1}\"\n",
        "            )\n",
        "\n",
        "            # Create audio\n",
        "            episode_file = f\"episode_{i+1:02d}.mp3\"\n",
        "            create_gtts_dual_voice_podcast(full_script, episode_file)\n",
        "\n",
        "            all_episodes.append(episode_file)\n",
        "            print(f\"‚úÖ Episode {i+1} saved as {episode_file}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error creating episode {i+1}: {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"\\nüéâ Created {len(all_episodes)} podcast episodes!\")\n",
        "    return all_episodes\n",
        "\n",
        "# ===============================\n",
        "# 7. Alternative: Generate Separate Audio Files\n",
        "# ===============================\n",
        "def create_multi_voice_podcast(script, output_dir=\"podcast_parts\"):\n",
        "    \"\"\"Split the script by speakers and create separate audio files\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # Split by speakers\n",
        "    parts = re.split(r'(\\*\\*(?:Host|Guest):\\*\\*)', script)\n",
        "\n",
        "    current_speaker = None\n",
        "    part_number = 1\n",
        "\n",
        "    for i, part in enumerate(parts):\n",
        "        if re.match(r'\\*\\*(Host|Guest):\\*\\*', part):\n",
        "            current_speaker = re.search(r'\\*\\*(Host|Guest):\\*\\*', part).group(1).lower()\n",
        "        elif part.strip() and current_speaker:\n",
        "            clean_part = clean_script_for_tts(part)\n",
        "            if clean_part:\n",
        "                filename = f\"{output_dir}/{part_number:02d}_{current_speaker}.mp3\"\n",
        "                try:\n",
        "                    tts = gTTS(text=clean_part, lang=\"en\")\n",
        "                    tts.save(filename)\n",
        "                    print(f\"‚úÖ Created: {filename}\")\n",
        "                    part_number += 1\n",
        "                except Exception as e:\n",
        "                    print(f\"‚ùå Error creating {filename}: {e}\")\n",
        "\n",
        "# ===============================\n",
        "# 8. Utility Functions\n",
        "# ===============================\n",
        "def check_dependencies():\n",
        "    \"\"\"Check if all required packages are installed\"\"\"\n",
        "    required_packages = {\n",
        "        'PyPDF2': 'PyPDF2',\n",
        "        'langchain': 'langchain',\n",
        "        'langchain_groq': 'langchain-groq',\n",
        "        'gtts': 'gTTS',\n",
        "        'pyttsx3': 'pyttsx3',\n",
        "        'pydub': 'pydub'\n",
        "    }\n",
        "\n",
        "    missing = []\n",
        "    for package, pip_name in required_packages.items():\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            missing.append(pip_name)\n",
        "\n",
        "    if missing:\n",
        "        print(\"‚ùå Missing packages:\")\n",
        "        for package in missing:\n",
        "            print(f\"   pip install {package}\")\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def list_available_voices():\n",
        "    \"\"\"List all available system voices\"\"\"\n",
        "    try:\n",
        "        engine = pyttsx3.init()\n",
        "        voices = engine.getProperty('voices')\n",
        "\n",
        "        if not voices:\n",
        "            print(\"‚ùå No voices available\")\n",
        "            return\n",
        "\n",
        "        print(\"üé≠ Available system voices:\")\n",
        "        for i, voice in enumerate(voices):\n",
        "            print(f\"   {i+1}. {voice.name} ({voice.id})\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error listing voices: {e}\")\n",
        "\n",
        "# ===============================\n",
        "# 9. Main Function\n",
        "# ===============================\n",
        "def main():\n",
        "    \"\"\"Main function to run the podcast generator\"\"\"\n",
        "    print(\"üéôÔ∏è PDF to Podcast Generator\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check dependencies\n",
        "    if not check_dependencies():\n",
        "        print(\"\\nüí° Please install missing packages and try again.\")\n",
        "        return\n",
        "\n",
        "    # Setup API key\n",
        "    GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\") or \"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\"\n",
        "    if not GROQ_API_KEY:\n",
        "        print(\"‚ùå Please set your GROQ_API_KEY as an environment variable\")\n",
        "        return\n",
        "\n",
        "    # Setup LLM\n",
        "    try:\n",
        "        llm = ChatGroq(\n",
        "            temperature=0.7,\n",
        "            groq_api_key=GROQ_API_KEY,\n",
        "            model_name=\"llama-3.3-70b-versatile\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error setting up LLM: {e}\")\n",
        "        return\n",
        "\n",
        "    # Get PDF path from user\n",
        "    pdf_path = input(\"üìÅ Enter PDF file path (or press Enter for default): \").strip()\n",
        "    if not pdf_path:\n",
        "        pdf_path = \"/content/NOTES UNIT-4 ANN.pdf\"  # Default path\n",
        "\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"‚ùå PDF file not found: {pdf_path}\")\n",
        "        return\n",
        "\n",
        "    # Load PDF and generate chunks\n",
        "    print(f\"\\nüìñ Loading PDF: {pdf_path}\")\n",
        "    raw_text = get_pdf_text(pdf_path)\n",
        "\n",
        "    if not raw_text:\n",
        "        print(\"‚ùå Could not extract text from PDF\")\n",
        "        return\n",
        "\n",
        "    chunks = get_chunks(raw_text)\n",
        "    print(f\"‚úÖ Created {len(chunks)} text chunks\")\n",
        "\n",
        "    # Choose operation mode\n",
        "    print(\"\\nüéØ Choose your mode:\")\n",
        "    print(\"1. Single episode (first chunk only)\")\n",
        "    print(\"2. Full podcast series (multiple episodes)\")\n",
        "    print(\"3. List available voices\")\n",
        "\n",
        "    mode = input(\"Enter choice (1-3) or press Enter for default (1): \").strip() or \"1\"\n",
        "\n",
        "    if mode == \"3\":\n",
        "        list_available_voices()\n",
        "        return\n",
        "\n",
        "    if mode == \"2\":\n",
        "        max_episodes = input(\"How many episodes? (1-10, default 3): \").strip()\n",
        "        try:\n",
        "            max_episodes = int(max_episodes) if max_episodes else 3\n",
        "            max_episodes = min(max_episodes, 10)  # Limit to 10\n",
        "        except:\n",
        "            max_episodes = 3\n",
        "\n",
        "        print(f\"\\nüé¨ Creating {max_episodes} episodes...\")\n",
        "        episodes = create_full_podcast_series(pdf_path, llm, max_episodes)\n",
        "        if episodes:\n",
        "            print(f\"\\nüéâ Successfully created {len(episodes)} episodes!\")\n",
        "            for ep in episodes:\n",
        "                print(f\"   üìÅ {ep}\")\n",
        "        return\n",
        "\n",
        "    # Single episode mode\n",
        "    print(\"\\nüéôÔ∏è Generating single episode from first chunk...\")\n",
        "\n",
        "    # Generate podcast script\n",
        "    podcast_script = generate_enhanced_podcast(chunks[0], llm)\n",
        "    full_script = add_podcast_intro_outro(podcast_script, \"Study Material\")\n",
        "\n",
        "    print(\"\\nüìù Generated Podcast Script:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(full_script[:500] + \"...\" if len(full_script) > 500 else full_script)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Automatically use dual voice with gTTS (male British + female Australian)\n",
        "    print(\"\\nüé≠ Creating podcast with dual voices:\")\n",
        "    print(\"   üßë Host (Sarah) - British English accent\")\n",
        "    print(\"   üë© Guest (Dr. Alex) - Australian English accent\")\n",
        "\n",
        "    try:\n",
        "        create_gtts_dual_voice_podcast(full_script, \"podcast_gtts_dual.mp3\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating audio: {e}\")\n",
        "        print(\"üí° Make sure all dependencies are installed properly\")\n",
        "\n",
        "    print(\"\\nüéâ Podcast generation complete!\")\n",
        "    print(\"\\nüí° Tips for better results:\")\n",
        "    print(\"- Use PDFs with clear, well-formatted text\")\n",
        "    print(\"- Install pydub for audio combination features\")\n",
        "    print(\"- Try different voice methods for best quality\")\n",
        "\n",
        "# ===============================\n",
        "# 10. Entry Point\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UK312NljQtrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SUMmarizer"
      ],
      "metadata": {
        "id": "I0UFrIZoQuSm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# ===============================\n",
        "# Setup Groq LLM\n",
        "# ===============================\n",
        "llm = ChatGroq(\n",
        "    temperature=0,\n",
        "    groq_api_key=\"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\",   # <-- replace with your key\n",
        "    model_name=\"meta-llama/llama-4-scout-17b-16e-instruct\"\n",
        ")\n",
        "\n",
        "# ===============================\n",
        "# Helper Functions\n",
        "# ===============================\n",
        "def load_pdf_text(pdf_paths):\n",
        "    \"\"\"Extract text from multiple PDFs.\"\"\"\n",
        "    text = \"\"\n",
        "    for path in pdf_paths:\n",
        "        reader = PdfReader(path.strip())\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    return text\n",
        "\n",
        "def chunk_text(text):\n",
        "    \"\"\"Split text into chunks for embeddings.\"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "    return splitter.split_text(text)\n",
        "\n",
        "def build_vectorstore(chunks):\n",
        "    \"\"\"Create FAISS vector DB from text chunks.\"\"\"\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    return FAISS.from_texts(chunks, embedding=embeddings)\n",
        "\n",
        "# ===============================\n",
        "# Summarizer\n",
        "# ===============================\n",
        "def summarize_pdf(pdf_paths, detail=False):\n",
        "    # 1. Load PDF\n",
        "    text = load_pdf_text(pdf_paths)\n",
        "    chunks = chunk_text(text)\n",
        "    vectorstore = build_vectorstore(chunks)\n",
        "\n",
        "    # 2. Create QA chain\n",
        "    qa = RetrievalQA.from_chain_type(\n",
        "        llm=llm,\n",
        "        retriever=vectorstore.as_retriever(),\n",
        "        chain_type=\"stuff\"\n",
        "    )\n",
        "\n",
        "    # 3. Ask summarization\n",
        "    if detail:\n",
        "        query = \"Summarize this document in detail with main topics and bullet points.\"\n",
        "    else:\n",
        "        query = \"Give a short summary of this document in one paragraph.\"\n",
        "\n",
        "    return qa.run(query)\n",
        "\n",
        "# ===============================\n",
        "# Run\n",
        "# ===============================\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_input = input(\"Enter PDF file paths (comma separated): \")\n",
        "    pdf_paths = [p.strip() for p in pdf_input.split(\",\")]\n",
        "\n",
        "    print(\"üìñ Generating Summary...\\n\")\n",
        "\n",
        "    short_summary = summarize_pdf(pdf_paths, detail=False)\n",
        "    detailed_summary = summarize_pdf(pdf_paths, detail=True)\n",
        "\n",
        "    print(\"‚úÖ Short Summary:\\n\", short_summary, \"\\n\")\n",
        "    print(\"‚úÖ Detailed Summary:\\n\", detailed_summary)\n"
      ],
      "metadata": {
        "id": "X1b2p8ScQxnj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}